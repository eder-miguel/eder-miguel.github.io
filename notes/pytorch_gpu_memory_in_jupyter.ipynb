{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd672b68-375b-43dd-bd9c-7c055fffbd16",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Pytorch GPU memory management in Jupyter\"\n",
    "description: \"Lessons learned about managing GPU memory when using Pytorch in Jupyter\"\n",
    "categories: [\"notes\", \"Pytorch\", \"Jupyter\"]\n",
    "format:\n",
    "  html: \n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a733d61",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "\n",
    "To clean-up cached memory in Pytorch:\n",
    "```python\n",
    "del x\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "```\n",
    "Note that `torch.cuda.empty_cache()` simply frees cached memory. For this to be truly effective, we must carefully handle variables and contexts, like we do in the previous example deleting variable `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c6368-6e2d-4385-a6cd-12c1528c53c1",
   "metadata": {},
   "source": [
    "Also, remember to the context manager when running the network in inference mode, e.g.:\n",
    "```python\n",
    "x, y = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    pred = net(x)\n",
    "```\n",
    "Running the code above without the context manager may result in difficulties freeing up the GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c36a4-39b1-40c6-a8ab-0d300668d772",
   "metadata": {},
   "source": [
    "To check the global namespace in a Jupyter environment:\n",
    "```\n",
    "%who\n",
    "```\n",
    "which will return all variables living in the global namespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3afc4-5786-4403-8913-fbc325d3876e",
   "metadata": {},
   "source": [
    "A more advanced approach would involve checking the garbage collector for tracked variables and referrers:\n",
    "```python\n",
    "import gc\n",
    "objs_in_gpu = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor) and obj.is_cuda]\n",
    "referrers = gc.get_referrers(objs_in_gpu[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c24d5",
   "metadata": {},
   "source": [
    "## Breakdown\n",
    "\n",
    "1. Python's [garbage collector](https://docs.python.org/3/library/gc.html) is the mechanism used in Python to determine when a variable (and the corresponding memory) can be freed. A reference count mechanism is used to keep track of references to memory. Sometimes, reference count increases can go unnoticed and create issues since the garbage collector does not free the memory as expected.\n",
    "2. A common error is storing references to Pytorch-related variables attached to computation graphs (as described [here](https://discuss.pytorch.org/t/memory-leak-from-unowned-inference/189575/2)). This will cause the computation graph to stay in memory and eventually trigger OOM errors or slow down processing due to inefficient memory management.\n",
    "3. A key pattern to avoid many of these errors is using context managers, which automatically handle reference counting and ensure that variables declared within the context are freed when the context is exited.\n",
    "```python\n",
    "with torch.cuda.device(0):\n",
    "    # Code\n",
    "```\n",
    "4. Functions and exception blocks (`try-except-finally`) also serve this purpose and it is often a good idea to combine them in functions that perform heavy processing tasks which might be interrupted. Interruptions, if not handled properly, may also interfere with the reference counting mechanism. The following is a useful snippet\n",
    "```python\n",
    "def func(params):\n",
    "    try:\n",
    "\t\t# Heavy processing here\n",
    "    except: # Use specific exception if expected\n",
    "\t\ttorch.cuda.empty_cache() # Ensure cached memory is freed if exception triggers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203ef1e",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Post](https://iifx.dev/en/articles/334554601) in `iifx.dev` discussing several methods to handle Out-Of-Memory errors associated to Pytorch.\n",
    "- [Post](https://www.neerjaaggarwal.com/post/memory-efficient-computing-using-pytorch-cuda-and-jupyter-notebooks) by Neerja Aggarwal describing OOM cases in the context of CUDA, Pytorch and Jupyter notebooks, and tips on avoiding them.\n",
    "- [Official Pytorch documentation](https://pytorch.org/docs/stable/notes/cuda.html#memory-management) on memory management when using CUDA.\n",
    "- [Post](https://mikulskibartosz.name/python-memory-management-in-jupyter-notebook) by Bartosz Mikulski discussing memory management in Jupyter notebooks.\n",
    "- [Stackoverflow thread](https://stackoverflow.com/questions/57858433/how-to-clear-gpu-memory-after-pytorch-model-training-without-restarting-kernel) with additional discussions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
